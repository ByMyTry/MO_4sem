{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import enjoyml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.decomposition import PCA\n",
    "# from joblib import dump, load\n",
    "\n",
    "# from data_engineering import read_data, get_filter_duplicates_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "set_session(tf.Session(config=config))\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool2D, Input, Dense, Dropout, Flatten, GlobalMaxPool2D\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras.optimizers import Adam\n",
    "from enjoyml.keras.layers import FixedPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'data/sign_mnist_train.csv'\n",
    "TEST_DATA_PATH = 'data/sign_mnist_test.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 28, 28, 1) (7172, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_data(df, img_size=28):\n",
    "    return df.iloc[:, 1:].values.reshape((df.shape[0], img_size, img_size, 1)), df['label'].values\n",
    "\n",
    "work_images, work_labels = get_data(df_train)\n",
    "test_images, test_labels = get_data(df_test)\n",
    "\n",
    "print(work_images.shape, test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24]),\n",
       " (24,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(work_labels), np.unique(work_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24709,) (2746,)\n"
     ]
    }
   ],
   "source": [
    "train_indixes, val_indexes = train_test_split(np.arange(work_images.shape[0]), \n",
    "                                              test_size=0.1, stratify=work_labels, random_state=42)\n",
    "\n",
    "print(train_indixes.shape, val_indexes.shape)\n",
    "train_images, train_labels = work_images[train_indixes], work_labels[train_indixes]\n",
    "val_images, val_labels = work_images[val_indexes], work_labels[val_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f045808ebe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0].reshape((28, 28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомная сверточная сеть, без аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28#150\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 48\n",
    "N_EPOCHS = 150\n",
    "\n",
    "IMAGES_TOTAL_COUNT = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 25)                3225      \n",
      "=================================================================\n",
      "Total params: 1,372,153\n",
      "Trainable params: 1,372,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input_ = x = Input((IMG_SIZE, IMG_SIZE, 1))\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(25, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=input_, outputs=output)\n",
    "\n",
    "model = get_model()\n",
    "model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enjoyml.multiclass import calc_class_weights\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rescale=1/255.,\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='loss', factor=5e-2, patience=3, \n",
    "                                         min_lr=1e-15, min_delta=0.01, verbose=1)\n",
    "stopper = callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=5, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                 )\n",
    "stopper_val = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 3.5174 - accuracy: 0.0619 - val_loss: 2.6352 - val_accuracy: 0.2052\n",
      "Epoch 2/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 1.7579 - accuracy: 0.4693 - val_loss: 0.3757 - val_accuracy: 0.8236\n",
      "Epoch 3/150\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.4346 - accuracy: 0.8635 - val_loss: 0.2053 - val_accuracy: 0.9350\n",
      "Epoch 4/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.1344 - accuracy: 0.9590 - val_loss: 0.0030 - val_accuracy: 0.9968\n",
      "Epoch 5/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0486 - accuracy: 0.9865 - val_loss: 0.0012 - val_accuracy: 0.9996\n",
      "Epoch 6/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 0.0038 - val_accuracy: 0.9988\n",
      "Epoch 7/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0299 - accuracy: 0.9925 - val_loss: 9.6677e-05 - val_accuracy: 0.9996\n",
      "Epoch 8/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.0017 - val_accuracy: 0.9996\n",
      "Epoch 9/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0126 - accuracy: 0.9968 - val_loss: 2.8110e-04 - val_accuracy: 0.9996\n",
      "Epoch 10/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 2.3355e-04 - val_accuracy: 0.9988\n",
      "Epoch 11/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 2.7896e-05 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "Epoch 12/150\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 2.8561e-05 - val_accuracy: 0.9996\n",
      "Epoch 13/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 1.0321e-05 - val_accuracy: 0.9996\n",
      "Epoch 14/150\n",
      "88/88 [==============================] - 3s 29ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.0968e-05 - val_accuracy: 0.9996\n",
      "Epoch 15/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 4.8502e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
      "Epoch 16/150\n",
      "88/88 [==============================] - 3s 29ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.8408e-05 - val_accuracy: 0.9996\n",
      "Epoch 17/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 3.2736e-05 - val_accuracy: 0.9996\n",
      "Epoch 18/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 8.2152e-06 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.2500000821091816e-07.\n",
      "Epoch 19/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.5084e-05 - val_accuracy: 0.9992\n",
      "Epoch 20/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 5.6771e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 8.4758e-06 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.250000694763003e-09.\n",
      "Epoch 22/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 1.9081e-05 - val_accuracy: 0.9996\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fbac02ba7b8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_STEPS_PER_EPOCH = int(IMAGES_TOTAL_COUNT * (1 - VALIDATION_SPLIT)/TRAIN_BATCH_SIZE) + 1\n",
    "VAL_STEPS_PER_EPOCH = int(IMAGES_TOTAL_COUNT * VALIDATION_SPLIT/VAL_BATCH_SIZE) + 1\n",
    "\n",
    "model.fit_generator(\n",
    "    datagen.flow(\n",
    "        train_images, train_labels,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    class_weight=calc_class_weights(train_labels),\n",
    "    validation_data=datagen.flow(\n",
    "        val_images, val_labels,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    callbacks=[lr_reducer, stopper, stopper_val], \n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14894315590616847, 0.9610987305641174]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомная сверточная сеть, с аугментацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28#150\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 48\n",
    "N_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                3225      \n",
      "=================================================================\n",
      "Total params: 1,372,153\n",
      "Trainable params: 1,372,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input_ = x = Input((IMG_SIZE, IMG_SIZE, 1))\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(25, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=input_, outputs=output)\n",
    "\n",
    "model = get_model()\n",
    "model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enjoyml.multiclass import calc_class_weights\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rescale=1/255.,\n",
    "    rotation_range=0.02,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.95, 1.05],\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "    fill_mode='reflect',\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='loss', factor=5e-2, patience=3, \n",
    "                                         min_lr=1e-15, min_delta=0.001, verbose=1)\n",
    "stopper = callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                 )\n",
    "stopper_val = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 3.6227 - accuracy: 0.0394 - val_loss: 3.1913 - val_accuracy: 0.0425\n",
      "Epoch 2/150\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 3.5929 - accuracy: 0.0463 - val_loss: 3.0197 - val_accuracy: 0.0930\n",
      "Epoch 3/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 2.8486 - accuracy: 0.1934 - val_loss: 1.6826 - val_accuracy: 0.4182\n",
      "Epoch 4/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 1.5033 - accuracy: 0.5175 - val_loss: 0.5609 - val_accuracy: 0.7538\n",
      "Epoch 5/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.5831 - accuracy: 0.8083 - val_loss: 0.1470 - val_accuracy: 0.9186\n",
      "Epoch 6/150\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 0.2293 - accuracy: 0.9313 - val_loss: 0.1694 - val_accuracy: 0.9501\n",
      "Epoch 7/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.1176 - accuracy: 0.9661 - val_loss: 0.0251 - val_accuracy: 0.9912\n",
      "Epoch 8/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0520 - accuracy: 0.9860 - val_loss: 0.0852 - val_accuracy: 0.9956\n",
      "Epoch 9/150\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.0403 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9968\n",
      "Epoch 10/150\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 0.0382 - accuracy: 0.9905 - val_loss: 0.0011 - val_accuracy: 0.9940\n",
      "Epoch 11/150\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 0.0387 - accuracy: 0.9905 - val_loss: 2.7539e-04 - val_accuracy: 0.9988\n",
      "Epoch 12/150\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 0.0252 - accuracy: 0.9935 - val_loss: 0.0056 - val_accuracy: 0.9969\n",
      "Epoch 13/150\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.0021 - val_accuracy: 0.9960\n",
      "Epoch 14/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0251 - accuracy: 0.9937 - val_loss: 6.4267e-05 - val_accuracy: 0.9956\n",
      "Epoch 15/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0225 - accuracy: 0.9945 - val_loss: 0.0692 - val_accuracy: 0.9880\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "Epoch 16/150\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 5.5044e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 3.2744e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 3.3304e-04 - val_accuracy: 0.9992\n",
      "Epoch 19/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 2.1657e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
      "Epoch 20/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 1.0425e-04 - val_accuracy: 0.9996\n",
      "Epoch 21/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 1.1734e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 1.2645e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2500000821091816e-07.\n",
      "Epoch 23/150\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 4.9759e-05 - val_accuracy: 0.9996\n",
      "Epoch 24/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 3.2844e-04 - val_accuracy: 1.0000\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fba1415f630>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_STEPS_PER_EPOCH = int(train_images.shape[0]/TRAIN_BATCH_SIZE) + 1\n",
    "VAL_STEPS_PER_EPOCH = int(val_images.shape[0]/VAL_BATCH_SIZE) + 1\n",
    "\n",
    "model.fit_generator(\n",
    "    datagen.flow(\n",
    "        train_images, train_labels,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    class_weight=calc_class_weights(train_labels),\n",
    "    validation_data=datagen.flow(\n",
    "        val_images, val_labels,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    callbacks=[lr_reducer, stopper, stopper_val], \n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00062408311848169, 0.9998605847358704]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение resnet сеть, с аугментацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28#150\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "N_EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_imgs_to_rgbs = lambda gray_imgs: np.repeat(gray_imgs, 3, -1)\n",
    "train_images_rgb = gray_imgs_to_rgbs(train_images)\n",
    "val_images_rgb = gray_imgs_to_rgbs(val_images)\n",
    "test_images_rgb = gray_imgs_to_rgbs(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f0458082080>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images_rgb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NET = True\n",
    "\n",
    "resnet_conv_base = ResNet50V2(\n",
    "    include_top=False, \n",
    "    weights='imagenet' if IMAGE_NET else None, \n",
    "    input_shape=(32, 32, 3)\n",
    ")  # imagenet\n",
    "\n",
    "\n",
    "def get_resnet_model():\n",
    "    input_ = Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    x = resnet_conv_base(input_)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "#     x = GlobalMaxPool2D()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.35)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(25, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    \n",
    "#     for layer in model.layers[1].layers[:-22]:\n",
    "    for layer in model.layers[1].layers[:-42]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "resnet_model = get_resnet_model()\n",
    "if IMAGE_NET:\n",
    "    #sparse_categorical_crossentropy\n",
    "    resnet_model.compile(Adam(lr=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    resnet_model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 3)         0         \n",
      "_________________________________________________________________\n",
      "resnet50v2 (Model)           (None, 1, 1, 2048)        23564800  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 25)                6425      \n",
      "=================================================================\n",
      "Total params: 25,931,801\n",
      "Trainable params: 18,191,385\n",
      "Non-trainable params: 7,740,416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enjoyml.multiclass import calc_class_weights\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rescale=1/255.,\n",
    "    rotation_range=0.02,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.95, 1.05],\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "    fill_mode='reflect',\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='loss', factor=5e-2, patience=3, \n",
    "                                         min_lr=1e-15, min_delta=0.001, verbose=1)\n",
    "stopper = callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                 )\n",
    "stopper_val = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "194/194 [==============================] - 12s 63ms/step - loss: 3.3684 - accuracy: 0.1555 - val_loss: 3.0121 - val_accuracy: 0.2225\n",
      "Epoch 2/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 1.9051 - accuracy: 0.5109 - val_loss: 1.5141 - val_accuracy: 0.4982\n",
      "Epoch 3/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 1.0778 - accuracy: 0.7087 - val_loss: 1.0777 - val_accuracy: 0.6162\n",
      "Epoch 4/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.7429 - accuracy: 0.7986 - val_loss: 0.7511 - val_accuracy: 0.7604\n",
      "Epoch 5/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.5718 - accuracy: 0.8454 - val_loss: 0.3871 - val_accuracy: 0.8740\n",
      "Epoch 6/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.4684 - accuracy: 0.8709 - val_loss: 0.2228 - val_accuracy: 0.9275\n",
      "Epoch 7/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.4082 - accuracy: 0.8911 - val_loss: 0.1399 - val_accuracy: 0.9264\n",
      "Epoch 8/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.3542 - accuracy: 0.9057 - val_loss: 0.3099 - val_accuracy: 0.9421\n",
      "Epoch 9/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.3174 - accuracy: 0.9149 - val_loss: 0.0672 - val_accuracy: 0.9541\n",
      "Epoch 10/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.3010 - accuracy: 0.9201 - val_loss: 0.0436 - val_accuracy: 0.9592\n",
      "Epoch 11/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.2802 - accuracy: 0.9262 - val_loss: 0.0647 - val_accuracy: 0.9621\n",
      "Epoch 12/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2517 - accuracy: 0.9349 - val_loss: 0.0564 - val_accuracy: 0.9629\n",
      "Epoch 13/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2232 - accuracy: 0.9398 - val_loss: 0.0485 - val_accuracy: 0.9741\n",
      "Epoch 14/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.2138 - accuracy: 0.9429 - val_loss: 0.0772 - val_accuracy: 0.9690\n",
      "Epoch 15/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2163 - accuracy: 0.9448 - val_loss: 0.0921 - val_accuracy: 0.9723\n",
      "Epoch 16/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2028 - accuracy: 0.9451 - val_loss: 0.0429 - val_accuracy: 0.9741\n",
      "Epoch 17/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1932 - accuracy: 0.9498 - val_loss: 0.1954 - val_accuracy: 0.9767\n",
      "Epoch 18/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1668 - accuracy: 0.9554 - val_loss: 0.0214 - val_accuracy: 0.9771\n",
      "Epoch 19/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1714 - accuracy: 0.9564 - val_loss: 0.0749 - val_accuracy: 0.9760\n",
      "Epoch 20/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1689 - accuracy: 0.9582 - val_loss: 0.0469 - val_accuracy: 0.9789\n",
      "Epoch 21/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1564 - accuracy: 0.9613 - val_loss: 0.1437 - val_accuracy: 0.9800\n",
      "Epoch 22/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1559 - accuracy: 0.9599 - val_loss: 0.0318 - val_accuracy: 0.9745\n",
      "Epoch 23/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1477 - accuracy: 0.9617 - val_loss: 0.0303 - val_accuracy: 0.9803\n",
      "Epoch 24/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1460 - accuracy: 0.9633 - val_loss: 0.0400 - val_accuracy: 0.9803\n",
      "Epoch 25/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1525 - accuracy: 0.9645 - val_loss: 0.1573 - val_accuracy: 0.9789\n",
      "Epoch 26/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1280 - accuracy: 0.9664 - val_loss: 0.0620 - val_accuracy: 0.9814\n",
      "Epoch 27/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1430 - accuracy: 0.9650 - val_loss: 0.0237 - val_accuracy: 0.9887\n",
      "Epoch 28/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1318 - accuracy: 0.9678 - val_loss: 0.0553 - val_accuracy: 0.9807\n",
      "Epoch 29/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1229 - accuracy: 0.9705 - val_loss: 0.0467 - val_accuracy: 0.9836\n",
      "Epoch 30/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1210 - accuracy: 0.9697 - val_loss: 0.0249 - val_accuracy: 0.9851\n",
      "Epoch 31/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1320 - accuracy: 0.9694 - val_loss: 0.0201 - val_accuracy: 0.9873\n",
      "Epoch 32/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1068 - accuracy: 0.9722 - val_loss: 0.0562 - val_accuracy: 0.9916\n",
      "Epoch 33/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1175 - accuracy: 0.9729 - val_loss: 0.0339 - val_accuracy: 0.9891\n",
      "Epoch 34/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1107 - accuracy: 0.9728 - val_loss: 0.0021 - val_accuracy: 0.9869\n",
      "Epoch 35/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1113 - accuracy: 0.9726 - val_loss: 0.0089 - val_accuracy: 0.9865\n",
      "Epoch 36/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1086 - accuracy: 0.9742 - val_loss: 0.0019 - val_accuracy: 0.9862\n",
      "Epoch 37/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.0957 - accuracy: 0.9779 - val_loss: 0.0440 - val_accuracy: 0.9876\n",
      "Epoch 38/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.0964 - accuracy: 0.9776 - val_loss: 0.0602 - val_accuracy: 0.9887\n",
      "Epoch 39/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.0915 - accuracy: 0.9775 - val_loss: 0.0623 - val_accuracy: 0.9858\n",
      "Epoch 40/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.0942 - accuracy: 0.9769 - val_loss: 0.0023 - val_accuracy: 0.9902\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 41/250\n",
      "194/194 [==============================] - 9s 49ms/step - loss: 0.0959 - accuracy: 0.9780 - val_loss: 0.0043 - val_accuracy: 0.9854\n",
      "Epoch 42/250\n",
      "194/194 [==============================] - 9s 49ms/step - loss: 0.0897 - accuracy: 0.9811 - val_loss: 0.0350 - val_accuracy: 0.9913\n",
      "Epoch 00042: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f675c7e67f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_STEPS_PER_EPOCH = int(train_images.shape[0]/TRAIN_BATCH_SIZE) + 1\n",
    "VAL_STEPS_PER_EPOCH = int(val_images.shape[0]/VAL_BATCH_SIZE) + 1\n",
    "\n",
    "resnet_model.fit_generator(\n",
    "    datagen.flow(\n",
    "        train_images_rgb, train_labels,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    class_weight=calc_class_weights(train_labels),\n",
    "    validation_data=datagen.flow(\n",
    "        val_images_rgb, val_labels,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    callbacks=[lr_reducer, stopper, stopper_val], \n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 3s 384us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17711056930343352, 0.959286093711853]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.evaluate(test_images_rgb/255, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общее сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14894315590616847, 0.9610987305641174]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00062408311848169, 0.9998605847358704]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 3s 384us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17711056930343352, 0.959286093711853]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.evaluate(test_images_rgb/255, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
