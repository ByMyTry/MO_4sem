{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import enjoyml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.decomposition import PCA\n",
    "# from joblib import dump, load\n",
    "\n",
    "# from data_engineering import read_data, get_filter_duplicates_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import backend as K\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "set_session(tf.Session(config=config))\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPool2D, Input, Dense, Dropout, Flatten, GlobalMaxPool2D\n",
    "from keras.applications.resnet_v2 import ResNet50V2\n",
    "from keras.optimizers import Adam\n",
    "from enjoyml.keras.layers import FixedPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models, optimizers, callbacks, regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'data/sign_mnist_train.csv'\n",
    "TEST_DATA_PATH = 'data/sign_mnist_test.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_DATA_PATH)\n",
    "df_test = pd.read_csv(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27455, 28, 28, 1) (7172, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_data(df, img_size=28):\n",
    "    return df.iloc[:, 1:].values.reshape((df.shape[0], img_size, img_size, 1)), df['label'].values\n",
    "\n",
    "work_images, work_labels = get_data(df_train)\n",
    "test_images, test_labels = get_data(df_test)\n",
    "\n",
    "print(work_images.shape, test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24]),\n",
       " (24,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(work_labels), np.unique(work_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24709,) (2746,)\n"
     ]
    }
   ],
   "source": [
    "train_indixes, val_indexes = train_test_split(np.arange(work_images.shape[0]), \n",
    "                                              test_size=0.1, stratify=work_labels, random_state=42)\n",
    "\n",
    "print(train_indixes.shape, val_indexes.shape)\n",
    "train_images, train_labels = work_images[train_indixes], work_labels[train_indixes]\n",
    "val_images, val_labels = work_images[val_indexes], work_labels[val_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомная сверточная сеть, без аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28#150\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 48\n",
    "N_EPOCHS = 150\n",
    "\n",
    "IMAGES_TOTAL_COUNT = 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_32 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 25)                3225      \n",
      "=================================================================\n",
      "Total params: 1,372,153\n",
      "Trainable params: 1,372,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input_ = x = Input((IMG_SIZE, IMG_SIZE, 1))\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(25, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=input_, outputs=output)\n",
    "\n",
    "model = get_model()\n",
    "model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enjoyml.multiclass import calc_class_weights\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rescale=1/255.,\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**data_gen_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='loss', factor=5e-2, patience=3, \n",
    "                                         min_lr=1e-15, min_delta=0.01, verbose=1)\n",
    "stopper = callbacks.EarlyStopping(monitor='loss', min_delta=0.001, patience=5, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                 )\n",
    "stopper_val = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "88/88 [==============================] - 3s 30ms/step - loss: 3.5174 - accuracy: 0.0619 - val_loss: 2.6352 - val_accuracy: 0.2052\n",
      "Epoch 2/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 1.7579 - accuracy: 0.4693 - val_loss: 0.3757 - val_accuracy: 0.8236\n",
      "Epoch 3/150\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.4346 - accuracy: 0.8635 - val_loss: 0.2053 - val_accuracy: 0.9350\n",
      "Epoch 4/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.1344 - accuracy: 0.9590 - val_loss: 0.0030 - val_accuracy: 0.9968\n",
      "Epoch 5/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0486 - accuracy: 0.9865 - val_loss: 0.0012 - val_accuracy: 0.9996\n",
      "Epoch 6/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0291 - accuracy: 0.9927 - val_loss: 0.0038 - val_accuracy: 0.9988\n",
      "Epoch 7/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0299 - accuracy: 0.9925 - val_loss: 9.6677e-05 - val_accuracy: 0.9996\n",
      "Epoch 8/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0150 - accuracy: 0.9959 - val_loss: 0.0017 - val_accuracy: 0.9996\n",
      "Epoch 9/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0126 - accuracy: 0.9968 - val_loss: 2.8110e-04 - val_accuracy: 0.9996\n",
      "Epoch 10/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 2.3355e-04 - val_accuracy: 0.9988\n",
      "Epoch 11/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0142 - accuracy: 0.9966 - val_loss: 2.7896e-05 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "Epoch 12/150\n",
      "88/88 [==============================] - 2s 26ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 2.8561e-05 - val_accuracy: 0.9996\n",
      "Epoch 13/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 1.0321e-05 - val_accuracy: 0.9996\n",
      "Epoch 14/150\n",
      "88/88 [==============================] - 3s 29ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.0968e-05 - val_accuracy: 0.9996\n",
      "Epoch 15/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0016 - accuracy: 0.9998 - val_loss: 4.8502e-06 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
      "Epoch 16/150\n",
      "88/88 [==============================] - 3s 29ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 1.8408e-05 - val_accuracy: 0.9996\n",
      "Epoch 17/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0012 - accuracy: 0.9999 - val_loss: 3.2736e-05 - val_accuracy: 0.9996\n",
      "Epoch 18/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 8.2152e-06 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 1.2500000821091816e-07.\n",
      "Epoch 19/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.5084e-05 - val_accuracy: 0.9992\n",
      "Epoch 20/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 5.6771e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/150\n",
      "88/88 [==============================] - 2s 28ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 8.4758e-06 - val_accuracy: 0.9996\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 6.250000694763003e-09.\n",
      "Epoch 22/150\n",
      "88/88 [==============================] - 2s 27ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 1.9081e-05 - val_accuracy: 0.9996\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fbac02ba7b8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_STEPS_PER_EPOCH = int(IMAGES_TOTAL_COUNT * (1 - VALIDATION_SPLIT)/TRAIN_BATCH_SIZE) + 1\n",
    "VAL_STEPS_PER_EPOCH = int(IMAGES_TOTAL_COUNT * VALIDATION_SPLIT/VAL_BATCH_SIZE) + 1\n",
    "\n",
    "model.fit_generator(\n",
    "    datagen.flow(\n",
    "        train_images, train_labels,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    class_weight=calc_class_weights(train_labels),\n",
    "    validation_data=datagen.flow(\n",
    "        val_images, val_labels,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    callbacks=[lr_reducer, stopper, stopper_val], \n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14894315590616847, 0.9610987305641174]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомная сверточная сеть, с аугментацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28#150\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 48\n",
    "N_EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 28, 28, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 7, 7, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 3, 3, 256)         295168    \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 25)                3225      \n",
      "=================================================================\n",
      "Total params: 1,372,153\n",
      "Trainable params: 1,372,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input_ = x = Input((IMG_SIZE, IMG_SIZE, 1))\n",
    "    \n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = MaxPool2D((2, 2))(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(25, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=input_, outputs=output)\n",
    "\n",
    "model = get_model()\n",
    "model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enjoyml.multiclass import calc_class_weights\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rescale=1/255.,\n",
    "    rotation_range=0.02,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.95, 1.05],\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "    fill_mode='reflect',\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='loss', factor=5e-2, patience=3, \n",
    "                                         min_lr=1e-15, min_delta=0.001, verbose=1)\n",
    "stopper = callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                 )\n",
    "stopper_val = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 3.6227 - accuracy: 0.0394 - val_loss: 3.1913 - val_accuracy: 0.0425\n",
      "Epoch 2/150\n",
      "88/88 [==============================] - 5s 53ms/step - loss: 3.5929 - accuracy: 0.0463 - val_loss: 3.0197 - val_accuracy: 0.0930\n",
      "Epoch 3/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 2.8486 - accuracy: 0.1934 - val_loss: 1.6826 - val_accuracy: 0.4182\n",
      "Epoch 4/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 1.5033 - accuracy: 0.5175 - val_loss: 0.5609 - val_accuracy: 0.7538\n",
      "Epoch 5/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.5831 - accuracy: 0.8083 - val_loss: 0.1470 - val_accuracy: 0.9186\n",
      "Epoch 6/150\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 0.2293 - accuracy: 0.9313 - val_loss: 0.1694 - val_accuracy: 0.9501\n",
      "Epoch 7/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.1176 - accuracy: 0.9661 - val_loss: 0.0251 - val_accuracy: 0.9912\n",
      "Epoch 8/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0520 - accuracy: 0.9860 - val_loss: 0.0852 - val_accuracy: 0.9956\n",
      "Epoch 9/150\n",
      "88/88 [==============================] - 5s 54ms/step - loss: 0.0403 - accuracy: 0.9890 - val_loss: 0.0015 - val_accuracy: 0.9968\n",
      "Epoch 10/150\n",
      "88/88 [==============================] - 4s 51ms/step - loss: 0.0382 - accuracy: 0.9905 - val_loss: 0.0011 - val_accuracy: 0.9940\n",
      "Epoch 11/150\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 0.0387 - accuracy: 0.9905 - val_loss: 2.7539e-04 - val_accuracy: 0.9988\n",
      "Epoch 12/150\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 0.0252 - accuracy: 0.9935 - val_loss: 0.0056 - val_accuracy: 0.9969\n",
      "Epoch 13/150\n",
      "88/88 [==============================] - 5s 52ms/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.0021 - val_accuracy: 0.9960\n",
      "Epoch 14/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0251 - accuracy: 0.9937 - val_loss: 6.4267e-05 - val_accuracy: 0.9956\n",
      "Epoch 15/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0225 - accuracy: 0.9945 - val_loss: 0.0692 - val_accuracy: 0.9880\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "Epoch 16/150\n",
      "88/88 [==============================] - 4s 47ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 5.5044e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 3.2744e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0047 - accuracy: 0.9991 - val_loss: 3.3304e-04 - val_accuracy: 0.9992\n",
      "Epoch 19/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 2.1657e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 2.5000001187436284e-06.\n",
      "Epoch 20/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 1.0425e-04 - val_accuracy: 0.9996\n",
      "Epoch 21/150\n",
      "88/88 [==============================] - 4s 49ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 1.1734e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/150\n",
      "88/88 [==============================] - 4s 46ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 1.2645e-05 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.2500000821091816e-07.\n",
      "Epoch 23/150\n",
      "88/88 [==============================] - 5s 51ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 4.9759e-05 - val_accuracy: 0.9996\n",
      "Epoch 24/150\n",
      "88/88 [==============================] - 4s 50ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 3.2844e-04 - val_accuracy: 1.0000\n",
      "Epoch 00024: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fba1415f630>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_STEPS_PER_EPOCH = int(train_images.shape[0]/TRAIN_BATCH_SIZE) + 1\n",
    "VAL_STEPS_PER_EPOCH = int(val_images.shape[0]/VAL_BATCH_SIZE) + 1\n",
    "\n",
    "model.fit_generator(\n",
    "    datagen.flow(\n",
    "        train_images, train_labels,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    class_weight=calc_class_weights(train_labels),\n",
    "    validation_data=datagen.flow(\n",
    "        val_images, val_labels,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    callbacks=[lr_reducer, stopper, stopper_val], \n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00062408311848169, 0.9998605847358704]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение resnet сеть, с аугментацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 28#150\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "N_EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray_imgs_to_rgbs = lambda gray_imgs: np.repeat(gray_imgs, 3, -1)\n",
    "train_images_rgb = gray_imgs_to_rgbs(train_images)\n",
    "val_images_rgb = gray_imgs_to_rgbs(val_images)\n",
    "test_images_rgb = gray_imgs_to_rgbs(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_NET = True\n",
    "\n",
    "resnet_conv_base = ResNet50V2(\n",
    "    include_top=False, \n",
    "    weights='imagenet' if IMAGE_NET else None, \n",
    "    input_shape=(32, 32, 3)\n",
    ")  # imagenet\n",
    "\n",
    "\n",
    "def get_resnet_model():\n",
    "    input_ = Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    \n",
    "    x = resnet_conv_base(input_)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "#     x = GlobalMaxPool2D()(x)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "#     x = Dense(2048, activation='relu')(x)\n",
    "#     x = Dropout(0.35)(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(25, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=input_, outputs=output)\n",
    "    \n",
    "#     for layer in model.layers[1].layers[:-22]:\n",
    "    for layer in model.layers[1].layers[:-42]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "resnet_model = get_resnet_model()\n",
    "if IMAGE_NET:\n",
    "    #sparse_categorical_crossentropy\n",
    "    resnet_model.compile(Adam(lr=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    resnet_model.compile('Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 28, 28, 3)         0         \n",
      "_________________________________________________________________\n",
      "resnet50v2 (Model)           (None, 1, 1, 2048)        23564800  \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 25)                6425      \n",
      "=================================================================\n",
      "Total params: 25,931,801\n",
      "Trainable params: 18,191,385\n",
      "Non-trainable params: 7,740,416\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enjoyml.multiclass import calc_class_weights\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict(\n",
    "    rescale=1/255.,\n",
    "    rotation_range=0.02,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=[0.95, 1.05],\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "    fill_mode='reflect',\n",
    ")\n",
    "\n",
    "datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "lr_reducer = callbacks.ReduceLROnPlateau(monitor='loss', factor=5e-2, patience=3, \n",
    "                                         min_lr=1e-15, min_delta=0.001, verbose=1)\n",
    "stopper = callbacks.EarlyStopping(monitor='loss', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                 )\n",
    "stopper_val = callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=10, \n",
    "                                  verbose=1#, restore_best_weights=True\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "194/194 [==============================] - 12s 63ms/step - loss: 3.3684 - accuracy: 0.1555 - val_loss: 3.0121 - val_accuracy: 0.2225\n",
      "Epoch 2/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 1.9051 - accuracy: 0.5109 - val_loss: 1.5141 - val_accuracy: 0.4982\n",
      "Epoch 3/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 1.0778 - accuracy: 0.7087 - val_loss: 1.0777 - val_accuracy: 0.6162\n",
      "Epoch 4/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.7429 - accuracy: 0.7986 - val_loss: 0.7511 - val_accuracy: 0.7604\n",
      "Epoch 5/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.5718 - accuracy: 0.8454 - val_loss: 0.3871 - val_accuracy: 0.8740\n",
      "Epoch 6/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.4684 - accuracy: 0.8709 - val_loss: 0.2228 - val_accuracy: 0.9275\n",
      "Epoch 7/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.4082 - accuracy: 0.8911 - val_loss: 0.1399 - val_accuracy: 0.9264\n",
      "Epoch 8/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.3542 - accuracy: 0.9057 - val_loss: 0.3099 - val_accuracy: 0.9421\n",
      "Epoch 9/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.3174 - accuracy: 0.9149 - val_loss: 0.0672 - val_accuracy: 0.9541\n",
      "Epoch 10/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.3010 - accuracy: 0.9201 - val_loss: 0.0436 - val_accuracy: 0.9592\n",
      "Epoch 11/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.2802 - accuracy: 0.9262 - val_loss: 0.0647 - val_accuracy: 0.9621\n",
      "Epoch 12/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2517 - accuracy: 0.9349 - val_loss: 0.0564 - val_accuracy: 0.9629\n",
      "Epoch 13/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2232 - accuracy: 0.9398 - val_loss: 0.0485 - val_accuracy: 0.9741\n",
      "Epoch 14/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.2138 - accuracy: 0.9429 - val_loss: 0.0772 - val_accuracy: 0.9690\n",
      "Epoch 15/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2163 - accuracy: 0.9448 - val_loss: 0.0921 - val_accuracy: 0.9723\n",
      "Epoch 16/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.2028 - accuracy: 0.9451 - val_loss: 0.0429 - val_accuracy: 0.9741\n",
      "Epoch 17/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1932 - accuracy: 0.9498 - val_loss: 0.1954 - val_accuracy: 0.9767\n",
      "Epoch 18/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1668 - accuracy: 0.9554 - val_loss: 0.0214 - val_accuracy: 0.9771\n",
      "Epoch 19/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1714 - accuracy: 0.9564 - val_loss: 0.0749 - val_accuracy: 0.9760\n",
      "Epoch 20/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1689 - accuracy: 0.9582 - val_loss: 0.0469 - val_accuracy: 0.9789\n",
      "Epoch 21/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1564 - accuracy: 0.9613 - val_loss: 0.1437 - val_accuracy: 0.9800\n",
      "Epoch 22/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1559 - accuracy: 0.9599 - val_loss: 0.0318 - val_accuracy: 0.9745\n",
      "Epoch 23/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1477 - accuracy: 0.9617 - val_loss: 0.0303 - val_accuracy: 0.9803\n",
      "Epoch 24/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1460 - accuracy: 0.9633 - val_loss: 0.0400 - val_accuracy: 0.9803\n",
      "Epoch 25/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1525 - accuracy: 0.9645 - val_loss: 0.1573 - val_accuracy: 0.9789\n",
      "Epoch 26/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1280 - accuracy: 0.9664 - val_loss: 0.0620 - val_accuracy: 0.9814\n",
      "Epoch 27/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1430 - accuracy: 0.9650 - val_loss: 0.0237 - val_accuracy: 0.9887\n",
      "Epoch 28/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1318 - accuracy: 0.9678 - val_loss: 0.0553 - val_accuracy: 0.9807\n",
      "Epoch 29/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1229 - accuracy: 0.9705 - val_loss: 0.0467 - val_accuracy: 0.9836\n",
      "Epoch 30/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1210 - accuracy: 0.9697 - val_loss: 0.0249 - val_accuracy: 0.9851\n",
      "Epoch 31/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1320 - accuracy: 0.9694 - val_loss: 0.0201 - val_accuracy: 0.9873\n",
      "Epoch 32/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1068 - accuracy: 0.9722 - val_loss: 0.0562 - val_accuracy: 0.9916\n",
      "Epoch 33/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1175 - accuracy: 0.9729 - val_loss: 0.0339 - val_accuracy: 0.9891\n",
      "Epoch 34/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.1107 - accuracy: 0.9728 - val_loss: 0.0021 - val_accuracy: 0.9869\n",
      "Epoch 35/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1113 - accuracy: 0.9726 - val_loss: 0.0089 - val_accuracy: 0.9865\n",
      "Epoch 36/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.1086 - accuracy: 0.9742 - val_loss: 0.0019 - val_accuracy: 0.9862\n",
      "Epoch 37/250\n",
      "194/194 [==============================] - 10s 50ms/step - loss: 0.0957 - accuracy: 0.9779 - val_loss: 0.0440 - val_accuracy: 0.9876\n",
      "Epoch 38/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.0964 - accuracy: 0.9776 - val_loss: 0.0602 - val_accuracy: 0.9887\n",
      "Epoch 39/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.0915 - accuracy: 0.9775 - val_loss: 0.0623 - val_accuracy: 0.9858\n",
      "Epoch 40/250\n",
      "194/194 [==============================] - 10s 49ms/step - loss: 0.0942 - accuracy: 0.9769 - val_loss: 0.0023 - val_accuracy: 0.9902\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n",
      "Epoch 41/250\n",
      "194/194 [==============================] - 9s 49ms/step - loss: 0.0959 - accuracy: 0.9780 - val_loss: 0.0043 - val_accuracy: 0.9854\n",
      "Epoch 42/250\n",
      "194/194 [==============================] - 9s 49ms/step - loss: 0.0897 - accuracy: 0.9811 - val_loss: 0.0350 - val_accuracy: 0.9913\n",
      "Epoch 00042: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f675c7e67f0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_STEPS_PER_EPOCH = int(train_images.shape[0]/TRAIN_BATCH_SIZE) + 1\n",
    "VAL_STEPS_PER_EPOCH = int(val_images.shape[0]/VAL_BATCH_SIZE) + 1\n",
    "\n",
    "resnet_model.fit_generator(\n",
    "    datagen.flow(\n",
    "        train_images_rgb, train_labels,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,\n",
    "    epochs=N_EPOCHS,\n",
    "    class_weight=calc_class_weights(train_labels),\n",
    "    validation_data=datagen.flow(\n",
    "        val_images_rgb, val_labels,\n",
    "        batch_size=VAL_BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "    ),\n",
    "    callbacks=[lr_reducer, stopper, stopper_val], \n",
    "    validation_steps=VAL_STEPS_PER_EPOCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 3s 384us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17711056930343352, 0.959286093711853]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.evaluate(test_images_rgb/255, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общее сравнение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14894315590616847, 0.9610987305641174]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 1s 80us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.00062408311848169, 0.9998605847358704]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images/255, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 3s 384us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17711056930343352, 0.959286093711853]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_model.evaluate(test_images_rgb/255, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
